# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12N9dXggp6lu2MDAdmjNRRqxwHryHBWyg
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from gensim.models import Word2Vec
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation as LDA

def load_data(uploaded_file):
    df = pd.read_csv(uploaded_file)
    return df.dropna()

def preprocess_data(df):
    X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['label'], test_size=0.2, random_state=42)
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)
    return X_train, X_test, y_train, y_test, label_encoder

def tokenize_data(X_train, X_test, max_words=10000, max_len=500):
    tokenizer = Tokenizer(num_words=max_words)
    tokenizer.fit_on_texts(X_train)
    train_sequences = tokenizer.texts_to_sequences(X_train)
    test_sequences = tokenizer.texts_to_sequences(X_test)
    train_data = pad_sequences(train_sequences, maxlen=max_len)
    test_data = pad_sequences(test_sequences, maxlen=max_len)
    return train_data, test_data, tokenizer

def train_word2vec(X_train):
    sentences = [text.split() for text in X_train]
    return Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

def create_embedding_matrix(tokenizer, w2v_model, max_words=10000, embedding_dim=100):
    word_index = tokenizer.word_index
    embedding_matrix = np.zeros((max_words, embedding_dim))
    for word, i in word_index.items():
        if i < max_words and word in w2v_model.wv:
            embedding_matrix[i] = w2v_model.wv[word]
    return embedding_matrix

def build_model(embedding_matrix, max_words=10000, max_len=500, embedding_dim=100):
    model = Sequential([
        Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),
        Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)),
        LSTM(64, dropout=0.3, recurrent_dropout=0.3),
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])
    return model

def analyze_clusters(negative_reviews):
    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
    X = vectorizer.fit_transform(negative_reviews['Review'])
    kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X)
    negative_reviews['Cluster'] = clusters
    return negative_reviews

def main():
    st.title("تحليل التعليقات باستخدام الذكاء الاصطناعي")
    uploaded_file = st.file_uploader("قم بتحميل ملف التعليقات (CSV)", type=["csv"])

    if uploaded_file is not None:
        df = load_data(uploaded_file)
        st.write(df.head())

        X_train, X_test, y_train, y_test, label_encoder = preprocess_data(df)
        train_data, test_data, tokenizer = tokenize_data(X_train, X_test)
        w2v_model = train_word2vec(X_train)
        embedding_matrix = create_embedding_matrix(tokenizer, w2v_model)

        model = build_model(embedding_matrix)
        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
        history = model.fit(train_data, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[early_stop])

        predictions = model.predict(test_data)
        predicted_labels = (predictions > 0.5).astype(int).flatten()

        st.write("### دقة النموذج:", accuracy_score(y_test, predicted_labels))
        st.write("### تقرير التصنيف:", classification_report(y_test, predicted_labels))

        df_negative = df.iloc[X_test.index][predicted_labels == 0]
        df_negative.to_csv("negative_reviews.csv", index=False)

        clustered_reviews = analyze_clusters(df_negative)
        st.write("### توزيع التجمعات السلبية:")
        st.write(clustered_reviews.head())

if __name__ == "__main__":
    main()